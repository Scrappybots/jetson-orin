## Global Args #################################################################
ARG BASE_UBI_IMAGE_TAG=9.4
ARG PYTHON_VERSION=3.11

# NOTE: This setting only has an effect when not using prebuilt-wheel kernels
ARG TORCH_CUDA_ARCH_LIST="7.2 8.7"


## Base Layer ##################################################################
FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} as base

WORKDIR /workspace

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

# Some utils for dev purposes - tar required for kubectl cp
RUN microdnf install -y \
      which \
      procps \
      findutils \
      tar \
      vim \
 && microdnf clean all

## Python Installer ############################################################
FROM base as python-install

ARG PYTHON_VERSION
ARG MINIFORGE_VERSION=24.3.0-0

RUN curl -fsSL -o ~/miniforge3.sh -O  "https://github.com/conda-forge/miniforge/releases/download/${MINIFORGE_VERSION}/Miniforge3-$(uname)-$(uname -m).sh" \
 && chmod +x ~/miniforge3.sh \
 && bash ~/miniforge3.sh -b -p /opt/conda \
 && source "/opt/conda/etc/profile.d/conda.sh" \
 && conda create -y -p /opt/vllm python=${PYTHON_VERSION} \
 && conda activate /opt/vllm \
 && rm ~/miniforge3.sh
# use of the /opt/vllm env requires:
# ENV PATH=/opt/vllm/bin/:$PATH

## CUDA Base ###################################################################
FROM base as cuda-base

# The Nvidia operator won't allow deploying on CUDA 12.0 hosts if
# this env var is set to 12.2.0, even though it's compatible
#ENV CUDA_VERSION=12.2.0 \
ENV CUDA_VERSION=12.0.0 \
    NV_CUDA_LIB_VERSION=12.2.0-1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    NV_CUDA_CUDART_VERSION=12.2.53-1 \
    NV_CUDA_COMPAT_VERSION=535.104.12

COPY overlays/cuda-repos/ /
RUN microdnf install -y \
      cuda-cudart-12-2-${NV_CUDA_CUDART_VERSION} \
      cuda-compat-12-2-${NV_CUDA_COMPAT_VERSION} \
 && microdnf clean all

ENV CUDA_HOME="/usr/local/cuda" \
    PATH="/usr/local/cuda/bin:${PATH}" \
    LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

## CUDA Runtime ################################################################
FROM cuda-base as cuda-runtime

ENV NV_NVTX_VERSION=12.2.53-1 \
    NV_LIBNPP_VERSION=12.1.1.14-1 \
    NV_LIBCUBLAS_VERSION=12.2.1.16-1 \
    NV_LIBNCCL_PACKAGE_VERSION=2.18.5-1+cuda12.2

RUN microdnf install -y \
      cuda-libraries-12-2-${NV_CUDA_LIB_VERSION} \
      cuda-nvtx-12-2-${NV_NVTX_VERSION} \
      libnpp-12-2-${NV_LIBNPP_VERSION} \
      libcublas-12-2-${NV_LIBCUBLAS_VERSION} \
      libnccl-${NV_LIBNCCL_PACKAGE_VERSION} \
 && microdnf clean all


## CUDA Development ############################################################
FROM cuda-runtime as cuda-devel

ENV NV_CUDA_CUDART_DEV_VERSION=12.2.53-1 \
    NV_NVML_DEV_VERSION=12.2.81-1 \
    NV_LIBCUBLAS_DEV_VERSION=12.2.1.16-1 \
    NV_LIBNPP_DEV_VERSION=12.1.1.14-1 \
    NV_LIBNCCL_DEV_PACKAGE_VERSION=2.18.5-1+cuda12.2

RUN microdnf install -y \
      cuda-command-line-tools-12-2-${NV_CUDA_LIB_VERSION} \
      cuda-libraries-devel-12-2-${NV_CUDA_LIB_VERSION} \
      cuda-minimal-build-12-2-${NV_CUDA_LIB_VERSION} \
      cuda-cudart-devel-12-2-${NV_CUDA_CUDART_DEV_VERSION} \
      cuda-nvml-devel-12-2-${NV_NVML_DEV_VERSION} \
      libcublas-devel-12-2-${NV_LIBCUBLAS_DEV_VERSION} \
      libnpp-devel-12-2-${NV_LIBNPP_DEV_VERSION} \
      libnccl-devel-${NV_LIBNCCL_DEV_PACKAGE_VERSION} \
      git

ENV LIBRARY_PATH="$CUDA_HOME/lib64/stubs"

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-12.2/compat/

## Custom compiled PyTorch ##########################################################
FROM cuda-devel as pytorch-build

COPY --from=python-install /opt/vllm /opt/vllm
ENV PATH=/opt/vllm/bin/:$PATH

ARG PYTORCH_BUILD_VERSION=2.3.1
ENV PYTORCH_BUILD_VERSION=${PYTORCH_BUILD_VERSION}

RUN git clone --recursive --branch v${PYTORCH_BUILD_VERSION} https://github.com/pytorch/pytorch

ARG TORCH_CUDA_ARCH_LIST
ENV TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST
#
# max jobs used by Ninja to build extensions
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads

RUN --mount=type=cache,target=/root/.cache/pip \
    microdnf install -y --enablerepo=codeready-builder-for-rhel-9-aarch64-rpms make cmake openblas-devel openmpi-devel \
 && cd pytorch \
 && pip3 install -r requirements.txt \
 && pip3 install scikit-build ninja

WORKDIR /workspace/pytorch

# Ref: https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048
ENV USE_NCCL=1 \
    USE_DISTRIBUTED=1 \
    USE_QNNPACK=0 \
    USE_PYTORCH_QNNPACK=0 \
    PYTORCH_BUILD_NUMBER=1

RUN --mount=type=cache,target=/root/.cache/pip \
    python3 setup.py bdist_wheel

## Python cuda base #################################################################
FROM cuda-devel as python-cuda-base

COPY --from=python-install /opt/vllm /opt/vllm
ENV PATH=/opt/vllm/bin/:$PATH

COPY --from=pytorch-build /workspace/pytorch/dist/ /opt/src/pytorch/

# install cuda and common dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements,target=requirements \
    pip3 install /opt/src/pytorch/*.whl \
 && pip3 install -r requirements/cuda.txt

## Development #################################################################
FROM python-cuda-base AS dev

# install build and runtime dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements,target=requirements \
    pip3 install -r requirements/dev.txt

## Proto Compilation ###########################################################
FROM python-install AS gen-protos

ENV PATH=/opt/vllm/bin/:$PATH

RUN microdnf install -y make

COPY protos protos

RUN --mount=type=cache,target=/root/.cache/pip \
    cd protos && make gen-protos

## Flash Attention build for aarch64 ############################################
FROM dev as flash-attn-build

ARG TORCH_CUDA_ARCH_LIST
ENV TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST
#
# max jobs used by Ninja to build extensions
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads


RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements,target=requirements \
    microdnf -y install git \
 && pip3 install -r requirements/build.txt

WORKDIR /usr/src/flash-attention-v2

RUN --mount=type=cache,target=/root/.cache/pip \
    pip --verbose wheel flash-attn==2.6.0.post1 --no-build-isolation --no-deps

RUN ls -halF && exit 1

## Builder #####################################################################
FROM dev AS build

# install build dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements,target=requirements \
    pip install -r requirements/build.txt

# copy input files
COPY overlays/vllm/build/ /
COPY --from=flash-attn-build /usr/src/flash-attention-v2/*.whl ./

ARG TORCH_CUDA_ARCH_LIST
ENV TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST

# max jobs used by Ninja to build extensions
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads
# make sure punica kernels are built (for LoRA)
ENV VLLM_INSTALL_PUNICA_KERNELS=1

# Setup path stuff? Ref: https://github.com/vllm-project/vllm/blob/main/.github/workflows/scripts/build.sh#L6-L8
ENV PATH=/usr/local/cuda/bin:$PATH
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Copy over the generated *.pb2 files
COPY --from=gen-protos /workspace/protos/vllm/entrypoints/grpc/pb vllm/entrypoints/grpc/pb

ENV CCACHE_DIR=/root/.cache/ccache
RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/pip \
    python3 setup.py bdist_wheel --dist-dir=dist

FROM cuda-runtime AS vllm-openai

WORKDIR /workspace

# Create release python environment
COPY --from=python-cuda-base /opt/vllm /opt/vllm
ENV PATH=/opt/vllm/bin/:$PATH

# install vllm wheel first, so that torch etc will be installed
RUN --mount=type=bind,from=build,src=/workspace/dist,target=/workspace/dist \
    --mount=type=cache,target=/root/.cache/pip \
    pip install dist/*.whl --verbose

# Install the vllm_nccl package which is a bit quirky
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=bind,source=requirements,target=requirements \
    # The "install" happens in `setup.py` so it happens when built...
    # Remove the already installed package and the cached wheel
    pip uninstall -y vllm-nccl-cu12 \
    && pip cache remove vllm_nccl* \
    # install the version depended on by vllm requirements
    && pip install vllm-nccl-cu12 -r requirements/cuda.txt \
    # The lib is downloaded to root's home directory... move it
    && mv ~/.config/vllm/nccl/cu12/libnccl.so.2* /usr/local/lib/libnccl.so.2
ENV VLLM_NCCL_SO_PATH=/usr/local/lib/libnccl.so.2

RUN --mount=type=cache,target=/root/.cache/pip \
    pip3 install \
        # additional dependencies for the TGIS gRPC server
        grpcio-tools \
        # additional dependencies for openai api_server
        accelerate \
        # hf_transfer for faster HF hub downloads
        hf_transfer

# Triton needs a CC compiler
RUN microdnf install -y gcc \
    && microdnf clean all

ENV HF_HUB_OFFLINE=1 \
    PORT=8000 \
    GRPC_PORT=8033 \
    HOME=/home/vllm \
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=fork

RUN ln -s /usr/lib64/libcuda.so.1 /usr/lib64/libcuda.so
COPY overlays/vllm/runtime/ /

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
